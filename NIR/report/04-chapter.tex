\chapter{Анализ предметной области}

\section{Регрессия}

\textbf{Определение:} Регрессия — это метод прогнозирования и анализа зависимости целевой переменной от одной или нескольких независимых переменных.
Этот подход широко применяется в задачах предсказания, моделирования и объяснения зависимости переменных, позволяя строить аналитические модели, описывающие взаимодействия в сложных системах~\cite{seber, montgomery}.

Модель регрессии предсказывает числовое значение.
Например, модель погоды, которая прогнозирует количество дождя в миллиметрах, является регрессионной моделью.

В таблице~\ref{tab:tabl1} приведены примеры регрессионных моделей.

\begin{table}[ht]
    \centering
    \caption{Примеры регрессионных моделей}
    \begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}
        \hline
        Сценарий & Входные данные & Выходные данные \newline(числовой прогноз) \\
        \hline
        Будущая цена дома & Площадь участка, количество спален и ванных комнат, размер участка, процентная ставка по ипотеке, ставка налога на недвижимость, затраты на строительство и количество домов, выставленных на продажу в этом районе & Цена дома \\
        \hline
        Будущее время поездки & Исторические условия дорожного движения, расстояние до пункта назначения и погодные условия & Время в минутах и секундах до прибытия в пункт назначения \\
        \hline
    \end{tabularx}
    \label{tab:tabl1}
\end{table}

Существует множество видов регрессии, и в данной работе будут рассмотрены следующие три вида регрессии: линейная регрессия, логистическая регрессия и адаптивная регрессия.

\chapter{Обзор существующих решений задачи регрессии}
\section{Линейная регрессия}

\textbf{ Определение:} Линейная регрессия --- это статистический метод, используемый для поиска взаимосвязи между переменными.
В контексте машинного обучения линейная регрессия находит связь между функциями и меткой~\cite{google}.

Линейная регрессия является одним из наиболее популярных алгоритмов и чаще всего используется для начала решения любой задачи регрессии, так как считается простейшей моделью машинного обучения~\cite{kemer}.

В математической статистике линейная регрессия представляет собой метод аппроксимации зависимостей между входными и выходными переменными на основе линейной модели~\cite{loginom}.

Если рассматривается зависимость между одной входной и одной выходной переменными, то имеет место простая линейная регрессия.
Уравнение регрессии для этого случая имеет вид:
\begin{equation}
    y = ax + b,
\end{equation}
где $a$ --- коэффициент наклона (или угловой коэффициент) линии регрессии, $b$ --- свободный член (перехват с осью $y$).

Коэффициенты $a$ и $b$, называемые также параметрами модели, определяются таким образом, чтобы сумма квадратов отклонений точек, соответствующих реальным наблюдениям данных, от линии регрессии была бы минимальной~\cite{loginom}.
Коэффициенты обычно оцениваются методом наименьших квадратов по следующей формуле:
\begin{equation}
   S(a, b) = \sum_{i=1}^{n}(y_i - (ax_i + b))^2,
\end{equation}
где $y_i$ --- наблюдаемое значение зависимой переменной для i-го наблюдения, $x_i$ --- значение независимой переменной для i-го наблюдения, $ax_i + b$ --- предсказанное значение зависимой переменной.

Чтобы найти коэффициенты $a$ и $b$, минимизируем эту сумму по отношению к $a$ и $b$.
Обычно, для этого используются аналитические формулы, полученные путем дифференцирования суммы квадратов остатков:
\begin{equation}
    a = \frac{n \sum_{i=1}^{n} x_i y_i - \sum_{i=1}^{n} x_i \sum_{i=1}^{n} y_i}{n \sum_{i=1}^{n} x_i^2 - (\sum_{i=1}^{n} x_i)^2},
\end{equation}
\begin{equation}
    b = \frac{1}{n}\sum_{i=1}^{n} y_i - a\frac{1}{n}\sum_{i=1}^{n} x_i,
\end{equation}
где $n$ --- количество наблюдений.

Для оценки качества модели линейной регрессии часто используется коэффициент детерминации $R^2$, который показывает долю изменчивости зависимой переменной, объясненную моделью.
Он рассчитывается как квадрат коэффициента корреляции $r_{xy}$:
\begin{equation}
    R^2 = r_{xy}^2,
\end{equation}
где $r_{xy}$ --- коэффициент корреляции между $x$ и $y$.
Чем ближе $R^2$ к 1, тем лучше модель объясняет зависимость между переменными.

На рисунке~\ref{img:linear-regression} представлен пример построения линии регрессии.
\includeimage
{linear-regression}
{f}
{H}
{\textwidth}
{Пример построения линии регрессии}

Если рассматривается зависимость между несколькими входными и одной выходной переменными, то имеет место множественная линейная регрессия.
Соответствующее уравнение имеет вид (\ref{eq:1}):
\begin{equation}
    y = b_0 + b_1 x_1 + b_2 x_2 + \cdots+ b_n x_n,
    \label{eq:1}
\end{equation}
где $n$ --- число входных переменных.

В данном случае модель будет описываться не прямой, а гиперплоскостью.
Коэффициенты уравнения множественной линейной регрессии подбираются так, чтобы минимизировать сумму квадратов отклонения реальных точек от этой гиперплоскости~\cite{loginom}.

\subsection*{Применение}

Линейная регрессия имеет много практических применений, которые можно разделить на две основные категории:
\begin{enumerate}[label=\arabic*), leftmargin=1.6\parindent]
    \item прогнозирование --- линейную регрессию можно использовать для подгонки модели к наблюдаемому набору данных;
    \item объяснение изменчивости --- линейный регрессионный анализ применяется для количественной оценки силы взаимосвязи между выходной и входными переменными.
\end{enumerate}

Оценим практическое применение способа построения линейной регрессии в экономике на примере формирования заработной платы, зависящей от показателя среднедушевого прожиточного минимума на человека, на основе представленных способов и формул.
В таблице~\ref{tab:tabl3} представлены данные, на базе которых нужно выявить зависимость показателя заработной платы от фактора $x$ по регионам России за 2016 год:

\begin{table}[H]
    \centering
    \caption{Показатели среднемесячной заработной платы под влиянием
    прожиточного минимума по регионам РФ за 2016 г., тыс. руб.}
    \begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}
        \hline
        № & Среднедушевой прожиточный min на одного работающего (х) & Среднемесячная заработная плата (у) \\
        \hline
        1 & 8.70 & 17.8 \\
        \hline
        2 & 6.30 & 11.6 \\
        \hline
        3 & 7.89 & 15.8 \\
        \hline
        4 & 10.24 & 13.5 \\
        \hline
        5 & 10.25 & 20.5 \\
        \hline
        6 & 7.50 & 15.9 \\
        \hline
        7 & 8.75 & 14.9 \\
        \hline
        8 & 6.20 & 10.3 \\
        \hline
        9 & 9.86 & 18.6 \\
        \hline
        10 & 8.50 & 14.2 \\
        \hline
    \end{tabularx}
    \label{tab:tabl3}
\end{table}

С помощью представленных данных построим линейное уравнений простой регрессии.
Для этого необходимо просчитать коэффициенты $a$ и $b$.
Для упрощения расчетов построим вспомогательную таблицу~\ref{tab:tabl4}:

\begin{table}[H]
    \centering
    \caption{Расчетная таблица параметров линейного уравнения регрессии}
        \begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}
        \hline
        № & $x$ & $y$ & $xy$ & $x^2$ & $y^2$ & $\hat{y}$ & $y-\hat{y}$ & $(y-\hat{y})^2$ & A (\%) \\
        \hline
        1 & 8.70 & 17.8 & 154.86 & 75.69 & 316.84 & 15.74 & 2.06 & 4.26 & 12\% \\
        \hline
        2 & 6.30 & 11.6 & 73.08 & 39.69 & 134.56 & 12.09 & -0.49 & 0.24 & 4\% \\
        \hline
        3 & 7.89 & 15.8 & 124.66 & 62.25 & 249.64 & 14.51 & 1.29 & 1.67 & 8\% \\
        \hline
        4 & 10.24 & 13.5 & 138.24 & 104.8 & 182.25 & 18.08 & -4.58 & 20.95 & 34\% \\
        \hline
        5 & 10.25 & 20.5 & 210.13 & 105 & 420.25 & 18.09 & 2.41 & 5.80 & 12\% \\
        \hline
        6 & 7.50 & 15.9 & 119.25 & 56.25 & 252.81 & 13.91 & 1.99 & 3.95 & 12\% \\
        \hline
        7 & 8.75 & 14.9 & 130.38 & 76.56 & 222.01 & 15.81 & -0.91 & 0.83 & 6\% \\
        \hline
        8 & 6.20 & 10.3 & 63.86 & 38.44 & 106.09 & 11.94 & -1.64 & 2.68 & 16\% \\
        \hline
        9 & 9.86 & 18.6 & 183.4 & 97.22 & 345.96 & 17.50 & 1.10 & 1.21 & 6\% \\
        \hline
        10 & 8.50 & 14.2 & 120.7 & 72.25 & 201.64 & 15.43 & -1.23 & 1.52 & 9\% \\
        \hline
        Сумма & 84.19 & 153.1 & 1318.55 & 728.27 & 2432.05 & 153.10 & 0.00 & 43.11 & 119\% \\
        \hline
        ср.знач. & 8.419 & 15.31 & 131.855 & 72.83 & 243.21 & 15.31 & 0.00 & 4.31 & 22\% \\
        \hline
    \end{tabularx}
    \label{tab:tabl4}
\end{table}

Для расчета коэффициентов $a$ и $b$ воспользуемся формулами:
\begin{itemize}
    \item $b$ (коэффициент наклона), вычисляется по формуле:
        \begin{equation}
            b = \frac{\sum (x_i - \overline{x})(y_i - \overline{y})}{\sum (x_i - \overline{x})},
        \end{equation}
    \item $a$ (свободный член), вычисляется через средние значения по формуле:
        \begin{equation}
            a = \overline{y} - b \cdot \overline{x},
        \end{equation}
\end{itemize}

Подставим значения в формулы:
\begin{equation}
    \overline{x} = \frac{\sum x}{n} = \frac{84.19}{10} = 8.42,
\end{equation}
\begin{equation}
    \overline{y} = \frac{\sum y}{n} = \frac{153.1}{10} = 15.31,
\end{equation}
\begin{equation}
    b = \frac{1318.55}{728.27} = 1.81,
\end{equation}
\begin{equation}
    a = 15.31 - 1.51 \cdot 8.42 = 2.52.
\end{equation}

Таким образом, уравнение регрессии имеет вид:
\begin{equation}
    y = 2.52 + 1.81x.
\end{equation}

Вычисляем для уравнения коэффициент детерминации:
\begin{equation}
    R^2 = \frac{(\sum(x_i - \overline{x})(y_i - \overline{y}))^2}{\sum (x_i - \overline{x})^2(y_i - \overline{y})^2} = \frac{1318.55^2}{728.27 \cdot 2432.05} = 0.51.
\end{equation}
Это означает, что 51\% изменчивости зарплаты можно объяснить изменчивостью прожиточного минимума.
Остальные 49\% объясняются другими факторами~\cite{plehanov}.

\section{Логистическая регрессия}

\textbf{ Определение:} Логистическая регрессия --- это статистическая модель, используемая для предсказания вероятности возникновения некоторого события путем подгонки данных к логистической кривой~\cite{kras}.

Логистическая регрессия применяется для предсказания вероятности возникновения некоторого события по значениям множества признаков.
Для этого вводится так называемая зависимая переменная $y$, принимающая лишь одно из двух значений, как правило, это числа: 0 (событие не произошло), и 1 (событие произошло), и множество независимых переменных (также называемых признаками, предикторами или регрессорами) --- вещественных $x_1, x_2, \cdots, x_n$, на основе значений которых требуется вычислить вероятность принятия того или иного значения зависимой переменной~\cite{kras}.

Стандартная логистическая функция, также известная как сигмовидная функция (\textit{сигмовидная} означает «s--образная»), имеет формулу (\ref{eq:2}):
\begin{equation}
    f(x) = \frac{1}{1 + e^{-x}}.
    \label{eq:2}
\end{equation}

На рисунке~\ref{img:sigmoid_function_with_axes} показан соответствующий график сигмовидной функции.
По мере увеличения входного значения $x$ выходной сигнал сигмовидной функции приближается, но никогда не достигает 1.
Точно так же, когда входные данные уменьшаются, выходные данные сигмовидной функции приближаются, но никогда не достигают 0.
\includeimage
{sigmoid_function_with_axes}
{f}
{H}
{\textwidth}
{График сигмовидной функции}

Линейный компонент модели логистической регрессии описывается следующим уравнением (\ref{eq:3}):
\begin{equation}
    z = b_0 + b_1 x_1 + b_2 x_2 +\cdots + b_n x_n,
    \label{eq:3}
\end{equation}
где $n$ --- число входных переменных, $z$ --- результат линейного уравнения (логарифм шансов), $b_i$ --- коэффициент регрессии для $i$--го признака, $x_i$ --- значения признаков.

Чтобы получить прогноз логистической регрессии, значение $z$ затем передается сигмовидной функции, что дает значение (вероятность) от 0 до 1 (формула~\ref{eq:4}):
\begin{equation}
    y' = \frac{1}{1 + e^{-z}},
    \label{eq:4}
\end{equation}
где $y'$ --- результат модели логистической регрессии, $z$ --- линейный выход (рассчитанный в уравнении~\ref{eq:3}).

На рисунке~\ref{img:linear_to_logistic} показано как линейный результат преобразуется в результат логистической регрессии.
\includeimage
{linear_to_logistic}
{f}
{H}
{\textwidth}
{Слева: график линейной функции $z = 2x + 5$, выделены три точки. Справа: сигмовидная кривая с теми же тремя точками, выделенными после преобразования сигмовидной функцией}

В качестве функции потерь в линейной регрессии используется метод наименьших квадратов (квадрат потерь).
Этот метод подходит для линейной модели, где скорость изменения выходных значений постоянна.
Например, для линейной модели $y' = b + 3 x_1$ каждый раз, когда увеличивается входное значение $x_1$ на 1, выходное значение $y'$ увеличивается на 3~\cite{google2}.

Однако скорость изменения модели логистической регрессии не является постоянной.
Когда значение логарифма шансов ($z$) ближе к 0, небольшое увеличение $z$ приводит к гораздо большим изменениям $y$, чем когда $z$ является большим положительным или отрицательным числом.

В таблице~\ref{tab:tabl2} показаны выходные данные сигмовидной функции для входных значений от 5 до 10, а также соответствующая точность, необходимая для учета различий в результатах.
\begin{table}[ht]
    \centering
    \caption{Выходные данные сигмовидной функции}
    \begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}
        \hline
        6 & 0.997 & 3 \\
        \hline
        7 & 0.999 & 3 \\
        \hline
        8 & 0.9997 & 4 \\
        \hline
        9 & 0.9999 & 4 \\
        \hline
        10 & 0.99998 & 5 \\
        \hline
    \end{tabularx}
    \label{tab:tabl2}
\end{table}

В данном случае нельзя успешно применить метод наименьших квадратов для оценки параметров $b$ и построения прогнозов, так как в этом случае прогнозные значения вероятности могут принимать как отрицательные значения, так и значения больше единицы~\cite{dbu}.
Поэтому для оценки коэффициентов модели используют метод максимального правдоподобия, который заключается в оценивании параметров путем максимизации функции правдоподобия.

Положительный коэффициент говорит о том, что данный фактор увеличивает общий риск, то есть повышает вероятность анализируемого исхода.
Отрицательный коэффициент означает, что данный фактор уменьшает риск, то есть понижает вероятность наступления исхода~\cite{dbu}.

\textbf{Определение:} Метод максимального правдоподобия --- еще один способ построения оценки неизвестного параметра.
Состоит он в том, что в качестве «наиболее правдоподобного» значения берут значение $\Theta$, максимизирующее вероятность получить при $n$ опытах данную выборку $X = (X_1, X_2, \cdots, X_n)$~\cite{dbu}.
Это значение параметра $\Theta$ зависит от выборки и является искомой оценкой.

Формула функции правдоподобия имеет вид:
\begin{equation}
    f(X, \Theta) = f_{\Theta}(X_1) \cdot f_{\Theta}(X_2) \cdot \cdots \cdot f_{\Theta}(X_n) = \prod_{i=1}^{n} f_{\Theta}(X_i).
\end{equation}

Формула логарифма правдоподобия имеет вид:
\begin{equation}
    L(X, \Theta) = \ln f(X, \Theta) = \sum_{i=1}^{n} \ln f_{\Theta}(X_i).
\end{equation}

\subsection*{Применение}

Согласно проведенному анализу современных исследований, связанных с использованием логистической регрессии, было выявлено несколько особенностей.
Во--первых, применение этой модели наиболее распространено в социально--экономических и медицинских исследованиях, хотя есть опыт применения в работах технического характера.
Во--вторых, с помощью этой модели решают три типа задач: прогнозирование, классификация и оценка влияния факторов на исход~\cite{vlasenko}.

При использовании логистической регрессии возникают следующие трудности:
\begin{enumerate}[label=\arabic*), leftmargin=1.6\parindent]
    \item ошибка соотнесения с классом значения --- возникает при классификации объектов, которые близки к границе класса (вероятность близка к 0.5).
    Например, в~\cite{simov} модель логистической регрессии используется для решения задачи кредитного скоринга.
    Задача заключается в классификации клиентов банка на два класса: надежные и ненадежные.
    Автор отмечает, что клиентов, для которых вероятность возвращения кредита близка к 0.5, невозможно классифицировать однозначно;
    \item мультиколлинеарность --- возникает, когда два или более предиктора в модели линейно зависимы.
    Например, в~\cite{muradov} отмечается, что пренебрежение зависимостями между независимыми переменными ведет к построению ошибочных моделей.
    Автор предлагает проводить предварительное статистическое исследование и исключать такие переменные из анализа;
    \item несбалансированные данные --- возникает, когда один из классов в обучающей выборке представлен значительно меньшим количеством объектов.
    Например, в работе~\cite{seredniy} автор отмечает, что при обучении модели на выборке с неравномерным распределением классов значений зависимой переменной была получена низкая точность прогноза.
    После «выравнивания» выборки и повторного обучения модель показала 99\% точности;
    \item проблема «границ чувствительности» --- возникает, когда модель логистической регрессии не может корректно оценить вероятность принадлежности объекта к классу.
    Например, в кредитном скоринге клиенты в возрасте 30 и 31 год практически одинаковые группы, а клиенты с возрастом 60 и 61 год --- весьма разные группы заемщиков~\cite{saponov}.
\end{enumerate}

Тем не менее большинство исследований показывают эффективность модели логистической регрессии.
Кроме высокой точности прогнозирования, стоит отметить ее достоинство решать задачи различного масштаба: от 3 независимых~\cite{simonova} переменных до 230~\cite{osikov}, от 40 записей~\cite{bogdanov} до 20 миллионов~\cite{seredniy}, а построенные модели понятны для интерпретации.

Несмотря на существующие ограничения, модель логистической регрессии показывает высокую точность прогнозирования и широкий спектр применения в различных предметных областях.


\section{Адаптивная регрессия}

\textbf{ Определение:} Адаптивная регрессия --- это метод статистического моделирования, который использует функции для представления нелинейных зависимостей между переменными.
В отличие от линейных моделей, адаптивная регрессия может автоматически подстраиваться под данные, включая нелинейные связи и взаимодействия между предикторами, что позволяет улучшить точность предсказаний~\cite{friedman2}.

Один из популярных методов адаптивной регрессии для выявления нелинейных связей в данных --- это многомерные адаптивные регрессионные сплайны (МАРС)~\cite{friedman2}.

МАРС --- это статистическая процедура, позволяющая решить классическую задачу регрессии: установить вид и параметры аппроксимирующей функции, описывающую функциональную зависимость отдельных наблюдений (исходные данные) с указанной точностью~\cite{romanova}.
Пространство значений входных переменных разбивается на области со своими собственными уравнениями базисных функций.
Это позволяет использовать МАРС даже в случае задач с «проклятием размерности», когда высокая размерность пространства значений входных переменных ограничивает применимость иных статистических процедур.

Метод МАР--сплайнов не имеет ограничения, характерного для иных статистических методов, в части наличия исходных предположений о типе зависимостей (линейных, степенных, экспоненциальных) между предикторными и выходными переменными~\cite{romanova}.
Кроме того, метод МАРС чувствителен к изменению вида связи между предиктором и откликом, будь то: изменение формы связи (например, от линейной к степенной), добавление или вычитание некоторой константы для прогноза отклика справа от узловой точки предиктора, изменение наклона регрессионной функции~\cite{romanova}.

Подобные особенности сплайнов достигаются за счет использования следующих базисных функций особого вида:
\begin{equation}
    (x - t)_{+} = \begin{cases}
        x - t, & если x > t, \\
        0, &~\text{иначе},
    \end{cases}
    \label{eq:5}
\end{equation}
\begin{equation}
    (t - x)_{-} = \begin{cases}
        t - x, & если x < t, \\
        0, &~\text{иначе},
    \end{cases}
    \label{eq:6}
\end{equation}
где $t$ --- точка разрыва (узел).
Этот метод оценивает каждую точку данных для каждого предиктора в качестве узла и создаёт линейную регрессионную модель с выбранной(ыми) переменной(ыми).

Формулы вида~(\ref{eq:5}~---~\ref{eq:6}) можно представить в следующем виде соответственно:
\begin{equation}
    (x - t)_{+} = max(0, x - t),
\end{equation}
\begin{equation}
    (t - x)_{+} = max(0, t - x).
\end{equation}

В многомерном случае для каждой компоненты $x_j$ вектора предикторов $x = (x_1, x_2, \cdots, x_n)$ строятся базисные функции вида~(\ref{eq:5}~---~\ref{eq:6}) с узлами в каждой наблюдаемой переменной $x_ij$, где $i = \overline{1, 2, \cdots, n}$ и $j = \overline{1, 2, \cdots, m}$.

Общее уравнение МАР--сплайнов для модели из $M$ членов, отличных от константы, представляет собой взвешенную сумму базисных функций и их произведений и записывается в виде~(\ref{eq:5}):
\begin{equation}
    y = f(x) = b_0 + \sum_{m=1}^{M} b_m h_m(x),
\end{equation}
где $b_0$ --- свободный член, $b_m$ --- коэффициенты регрессии, определяемые методом наименьших квадратов, $h_m(x)$ --- базисная функция, $M$ --- число базисных функций.

Основной принцип работы модели состоит в выборе нужной взвешенной суммы базисных функций из общего набора базисных функций, покрывающих все значения каждого предиктора (т.е. набор будет состоять из одной базисной функции и параметра $t$ для каждого отдельного значения каждой предикторной переменной).
Алгоритм МАР--сплайнов отыскивает в пространстве всех входных и предикторных переменных расположение узловых точек, а также взаимосвязи между переменными~\cite{romanova}.
В процессе поиска число добавленных к модели базисных функций из общего набора возрастает до тех пор, пока не будет максимизирован общий критерий качества модели --- обобщенное скользящее среднее, который имеет следующий вид:
\begin{equation}
    GCV(M) = \frac{1}{n} \frac{\sum_{i=1}^{n} (y_i - f(x_i))^2}{(1 - \frac{C}{n})^2},
\end{equation}
где $GCV(M)$ --- критерий точности модели, отражающий рост дисперсии с ростом числа базисных функций, $C$ --- эффективное число параметров модели, которое в общем случае определяется как $C = 2K - 1$, где $K$ --- общее число параметров модели.

Пошаговый алгоритм построения МАР--сплайна схож с алгоритмом линейной регрессии, только вместо регрессионных функций используются базисные функции.
Например, рассмотрим нелинейные, немонотонные данные, где $y = f(x)$.
Процедура МАРС сначала ищет одну точку в диапазоне значений $x$, где две различные линейные зависимости между $y$ и $x$ минимизируют ошибку методом наименьших квадратов~\cite{gitbook}.

Результатом становится функция $h(x - a)$, где $a$ --- это значение точки разрыва.
Для одного узла функция $h(x - a)$ выглядит как $h(x - 1.183606)$, следовательно, две линейные модели для $y$ имеют вид:
\begin{equation}
  \text{y} =
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606
  \end{cases}
\end{equation}

После нахождения первого узла поиск продолжается для второго узла, который обнаруживается при $x = 4.898114$.
Это приводит к созданию трёх линейных моделей для $y$:

\begin{equation}
  \text{y} =
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606 \quad \& \quad \text{x} < 4.898114, \\
    \beta_0 + \beta_1(4.898114 - \text{x}) & \text{x} > 4.898114
  \end{cases}
\end{equation}

На рисунке~\ref{img:examples-of-multiple-knots-1.png} показан пример адаптивного регрессионного сплайна с одним, двумя, тремя и четырьмя узлами соответственно.
\includeimage
{examples-of-multiple-knots-1.png}
{f}
{H}
{\textwidth}
{Пример адаптивного регрессионного сплайна}

Этот процесс продолжается до тех пор, пока не будет найдено множество узлов, что приводит к созданию точного нелинейного уравнения предсказания.
Добавление большого количества узлов может позволить модели полностью соответствовать обучающим данным, но привести к недостаточной обобщаемости на новые, ранее не виденные данные.

Поэтому, после того как полный набор узлов найден, следует поочередно удалять узлы, которые не вносят значительного вклада в точность предсказания~\cite{gitbook}.

\subsection*{Применение}

МАР--сплайны находят свое применение во многих сферах науки и технологий, например, в предсказании видов распределений по имеющимся данным~\cite{elith2007}, кишечного поглощения лекарств~\cite{deconinck2007}, а также в воспроизведении речи~\cite{haas1998} и поиске глобального оптимума в проектировании конструкций~\cite{crino2007}.

Многомерные адаптивные регрессионные сплайны обладают рядом преимуществ перед другими регрессионными методами~\cite{fedosin}:
\begin{enumerate}[label=\arabic*), leftmargin=1.6\parindent]
    \item модели, построенные с использованием МАР--сплайнов, обладают большей гибкостью, чем модели, построенные при помощи линейной регрессии;
    \item МАР--сплайны могут автоматически находить нелинейные зависимости между переменными, что позволяет улучшить точность прогнозов;
    \item МАР--сплайны позволяют работать с численными и категориальными признаками;
    \item благодаря разделению исходных данных на области базисными функциями, МАР--сплайны позволяют определять выбросы;
    \item МАР--сплайны не требуют значительных мер по подготовке входных данных;
    \item метод демонстрирует высокую устойчивость к многоколлинеарности, что особенно важно при работе с большими наборами данных;
\end{enumerate}

Благодаря своим преимуществам, МАР--сплайны нашли применение в биоинформатике для анализа геномных данных, включая прогнозирование экспрессии генов и идентификацию биомаркеров.
Метод активно используется в экологии для моделирования и прогнозирования ареалов обитания видов с учетом множества факторов окружающей среды, в экономике и финансах --- для прогнозирования цен, анализа временных рядов и моделирования волатильности;
В задачах управления производственными процессами МАР--сплайны помогают оптимизировать технологические параметры и контролировать качество продукции.
